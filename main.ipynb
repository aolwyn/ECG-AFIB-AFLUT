{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline File\n",
    "\n",
    "the following will be the pipeline:\n",
    "<ol>\n",
    "   <li>Read in the files and deal with the missing data. </li>\n",
    "   <li>Preprocess the signals</li>\n",
    "   <li>Complete feature extraction</li>\n",
    "   <li>Put data into a model.</li>\n",
    "   <li>optimize, compare, iterate - try other models.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries. \n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import wfdb\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from scipy.signal import butter, lfilter\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load helper files.\n",
    "import dataloaders\n",
    "import visualize\n",
    "import preprocess\n",
    "import segment \n",
    "# import cart_model # @Henry what is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders.get_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'C:/Users/henry/OneDrive/Desktop/ELEC 872 - AI and Interactive Systems/Project/mit-bih-arrhythmia-database-1.0.0/'\n",
    "# 'G:/Datasets/mit-bih-arrhythmia-database-1.0.0/'\n",
    "file_path = 'G:/Datasets/mit-bih-arrhythmia-database-1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. \n",
    "patient_data = dataloaders.load_all_records(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "# Print the structure of the patient_data dictionary\n",
    "pp.pprint(patient_data['103'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the rhythm count (as beats per rhythm type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range (100,125):\n",
    "#     visualize.summarize_rhythm_counts(patient_data, str(i))\n",
    "\n",
    "# for i in range (200,225):\n",
    "#     visualize.summarize_rhythm_counts(patient_data, str(i))\n",
    "\n",
    "#visualize.summarize_rhythm_counts(patient_data, \"203\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "\n",
    "Note. Prior to this, we downsample. typically we downsample AFTER preprocessing. I've elected to do it before because of the way the annotation object is structured.\n",
    "\n",
    "0. Convert Dictionary to array value style for use with the filter functions from scipy.\n",
    "1. High-Pass Filter to remove baseline wander\n",
    "2. Notch Filter to remove powerline interference (if any?)\n",
    "3. Low-Pass Filter to remove high-frequency noise (set for 40 hz for now?)\n",
    "4. Moving Average Filter to smooth the remaining signal \n",
    "5. Normalization for 0-->1 because the leads all act differently\n",
    "\n",
    "<p> may want to consider an FFT or a Wavelet Transform because it's time series data. can determine later on. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data into arrays from the dict first, note this is a progress test.\n",
    "# aggregated_patient_data = preprocess.aggregate_signals(patient_data)\n",
    "# pp.print(aggregated_data['103'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocess.preprocess_patient_data(patient_data)\n",
    "pp.pprint(processed_data['103'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without labels. \n",
    "visualize.visualize_patient_data(processed_data,'100',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with labels - note i just did very basic math and put it in the general area of the segment. might not lineup 1:1\n",
    "visualize.visualize_patient_data_with_rhythm(processed_data, patient_id='103', display_seconds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractions?\n",
    "\n",
    "Now that the signals have been filtered and normalized - we have 2 leads. we can select which one is more important to use.\n",
    "considerations include ICA, correlation, SNR etc.\n",
    "\n",
    "for now, I've done SNR, std deviation, high frequency check, entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the best lead's signal and labels for each patient\n",
    "best_leads_data = {}\n",
    "\n",
    "# Iterate through all patients and determine the better lead for each\n",
    "for patient_id in processed_data.keys():\n",
    "    # Determine the best lead\n",
    "    better_lead = visualize.compute_noise_metrics_for_patient(processed_data, patient_id)\n",
    "    print(f\"Patient {patient_id}: Best Lead is {better_lead}\")\n",
    "\n",
    "    # Extract the corresponding lead's signal and labels , \n",
    "    if better_lead == 'Lead 1':\n",
    "        best_signal = processed_data[patient_id]['signals_lead_1']\n",
    "    elif better_lead == 'Lead 2':\n",
    "        best_signal = processed_data[patient_id]['signals_lead_2']\n",
    "\n",
    "    labels = processed_data[patient_id]['labels']\n",
    "\n",
    "    # Save the best signal and labels into the dictionary\n",
    "    best_leads_data[patient_id] = {\n",
    "        'signal': best_signal,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"new dictionary with specifc labels + lead for given patient added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test simple RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for segmentation\n",
    "segment_length_sec = 10  # Segment length in seconds\n",
    "fs = 250  # Sampling frequency\n",
    "\n",
    "# Prepare segmented data\n",
    "rnn_signals = []\n",
    "rnn_labels = []\n",
    "\n",
    "for patient_id, data in best_leads_data.items():\n",
    "    signal = data['signal']  # The best lead's signal\n",
    "    labels = data['labels']  # Corresponding labels\n",
    "\n",
    "    # Segment the data\n",
    "    segments, segment_labels = segment.prepare_rnn_data(signal, labels, segment_length_sec, fs)\n",
    "    rnn_signals.extend(segments)\n",
    "    rnn_labels.extend(segment_labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "rnn_signals = np.array(rnn_signals)\n",
    "rnn_labels = np.array(rnn_labels)\n",
    "\n",
    "print(f\"RNN Signals Shape: {rnn_signals.shape}\")\n",
    "print(f\"RNN Labels Shape: {rnn_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "data_splits = segment.split_data(rnn_signals, rnn_labels)\n",
    "\n",
    "# Create DataLoaders for PyTorch\n",
    "train_loader, val_loader, test_loader = dataloaders.create_dataloaders(data_splits, batch_size=32)\n",
    "\n",
    "print(f\"Train Loader Size: {len(train_loader)} batches\")\n",
    "print(f\"Validation Loader Size: {len(val_loader)} batches\")\n",
    "print(f\"Test Loader Size: {len(test_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ECG_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ECG_RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Take the last hidden state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Val Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # ECG data is 1D\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = len(set(rnn_labels))\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model\n",
    "model = ECG_RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_loader, val_loader, test_loader = dataloaders.create_dataloaders(data_splits, batch_size)\n",
    "model = train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
