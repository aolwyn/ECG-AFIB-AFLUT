{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline File\n",
    "\n",
    "the following will be the pipeline:\n",
    "<ol>\n",
    "   <li>Read in the files and deal with the missing data. </li>\n",
    "   <li>Preprocess the signals</li>\n",
    "   <li>Complete feature extraction</li>\n",
    "   <li>Put data into a model.</li>\n",
    "   <li>optimize, compare, iterate - try other models.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries. \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import wfdb\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from scipy.signal import butter, lfilter\n",
    "import pprint\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load helper files.\n",
    "import dataloaders\n",
    "import visualize\n",
    "import preprocess\n",
    "import segment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders.get_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'C:/Users/henry/OneDrive/Desktop/ELEC 872 - AI and Interactive Systems/Project/mit-bih-arrhythmia-database-1.0.0/'\n",
    "# 'G:/Datasets/mit-bih-arrhythmia-database-1.0.0/'\n",
    "# 'D:/Datasets/mit-bih-arrhythmia-database-1.0.0/'\n",
    "file_path = 'G:/Datasets/mit-bih-arrhythmia-database-1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. \n",
    "patient_data = dataloaders.load_all_records(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "# Print the structure of the patient_data dictionary\n",
    "pp.pprint(patient_data['103'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the rhythm count (as beats per rhythm type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range (100,125):\n",
    "#     visualize.summarize_rhythm_counts(patient_data, str(i))\n",
    "\n",
    "# for i in range (200,225):\n",
    "#     visualize.summarize_rhythm_counts(patient_data, str(i))\n",
    "\n",
    "#visualize.summarize_rhythm_counts(patient_data, \"203\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "\n",
    "Note. Prior to this, we downsample. typically we downsample AFTER preprocessing. I've elected to do it before because of the way the annotation object is structured.\n",
    "\n",
    "<h2>Method 1</h2>\n",
    "\n",
    "0. Convert Dictionary to array value style for use with the filter functions from scipy.\n",
    "1. High-Pass Filter to remove baseline wander\n",
    "2. Notch Filter to remove powerline interference (if any?)\n",
    "3. Low-Pass Filter to remove high-frequency noise (set for 40 hz for now?)\n",
    "4. Moving Average Filter to smooth the remaining signal \n",
    "5. Normalization for 0-->1 because the leads all act differently\n",
    "\n",
    "<h2>Method 2</h2>\n",
    "\n",
    "0. Convert Dictionary to array value style for use with the filter functions from the pywavelet\n",
    "1. apply wavelet using the equation + methods from the paper\n",
    "2. normalization for 0-->1\n",
    "\n",
    "<p> @TODO: possibly a dual channel arch. if we have the time, implement the feature creation (R-R, HRV, beat) that we have already created</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data into arrays from the dict first, note this is a progress test.\n",
    "# aggregated_patient_data = preprocess.aggregate_signals(patient_data)\n",
    "# pp.print(aggregated_data['103'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and print out. should all be normal for first few samples shown.\n",
    "processed_data = preprocess.preprocess_patient_data(patient_data)\n",
    "pp.pprint(processed_data['103'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without labels. \n",
    "# visualize.visualize_patient_data(processed_data,'100',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with labels - note i just did very basic math and put it in the general area of the segment. might not lineup 1:1\n",
    "# visualize.visualize_patient_data_with_rhythm(processed_data, patient_id='103', display_seconds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractions, Selection\n",
    "\n",
    "<p> regarding featuere extraction and selection. We've got HRV, R-R peak calculations. do we want to actually use? </p>\n",
    "\n",
    "- P.S., other models performing fine w/out, can try if we have time. If not, that's in our next steps / how to improve.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>Now that the signals have been filtered and normalized - we have 2 leads. we can select which one is more important to use. The database website on PhysioNet recommends certain leads for specific tasks, however, in our case, we want generalizability - so we will consider the lead that has the most information / least noise.\n",
    "</p>\n",
    "\n",
    "Considerations are as follows and are weighted based on what we felt was best:\n",
    "\n",
    "1. Signal Noise Ratio (SNR)\n",
    "2. Entropy\n",
    "3. Standard Deviation\n",
    "4. High Frequency Power\n",
    "5. ICA\n",
    "\n",
    "Other considerations were made and baselines were tested, however, we felt that these 4/5 had the highest impact. The leads not chosen were tested however and results were similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the best lead's signal and labels for each patient\n",
    "best_leads_data = {}\n",
    "\n",
    "# Iterate through all patients and determine the better lead for each\n",
    "for patient_id in processed_data.keys():\n",
    "\n",
    "    better_lead = visualize.compute_noise_metrics_for_patient(processed_data, patient_id)\n",
    "    # print(f\"Patient {patient_id}: Best Lead is {better_lead}\")\n",
    "\n",
    "    # Extract the corresponding lead's signal and labels , \n",
    "    if better_lead == 'Lead 1':\n",
    "        best_signal = processed_data[patient_id]['signals_lead_1']\n",
    "    elif better_lead == 'Lead 2':\n",
    "        best_signal = processed_data[patient_id]['signals_lead_2']\n",
    "\n",
    "    labels = processed_data[patient_id]['labels']\n",
    "\n",
    "    best_leads_data[patient_id] = {\n",
    "        'signal': best_signal,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"new dictionary with specifc labels + lead for given patient added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models\n",
    "\n",
    "<p><strong>Note:</strong> See the sub folder/module, named models, for individual model information. Some use the same train file (see reshape parameter in model class). Some have their own train. They all use the same preprocessing pipeline (with some minor adjustments to assist with the shape.) </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "``` \n",
    "LABEL_ENCODING = \n",
    "{\n",
    "        'normal': 0,\n",
    "        'atrial_fibrillation': 1,\n",
    "        'atrial_flutter': 2,\n",
    "        'ventricular_bigeminy': 3,\n",
    "        'Other': 4 \n",
    "}\n",
    "```\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries from the model sub folder\n",
    "from models.rnn import ECG_RNN\n",
    "# from models.rnn2 import RNN2\n",
    "from models.resnet import ResNet1D\n",
    "from models.cnnlstm import CNNLSTM\n",
    "from models.seq2seq import ECGSeq2Seq\n",
    "\n",
    "from train import train_model, evaluate_model\n",
    "from traincnnlstm import train_cnn_lstm\n",
    "\n",
    "# functions from the reg files\n",
    "from segment import prepare_rnn_data, split_data\n",
    "from dataloaders import create_dataloaders\n",
    "# from preprocess import encode_labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # pretty sure need to re-add this, didnt use the one I used from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(signals, labels, label_encoding, max_normal_samples=1500):\n",
    "    \"\"\"\n",
    "    Balances the dataset by taking a fixed number of samples from the 'normal' class.\n",
    "\n",
    "    Args:\n",
    "        signals (np.array): The segmented signal data.\n",
    "        labels (np.array): The encoded labels corresponding to the signals.\n",
    "        label_encoding (dict): Dictionary mapping class names to numeric labels.\n",
    "        max_normal_samples (int): Fixed number of samples to take from the 'normal' class.\n",
    "\n",
    "    Returns:\n",
    "        balanced_signals (np.array): Subsampled signal data.\n",
    "        balanced_labels (np.array): Corresponding labels after subsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count class distribution\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    # Identify indices\n",
    "    normal_label = label_encoding['normal']\n",
    "    normal_indices = np.where(labels == normal_label)[0]\n",
    "    other_indices = np.where(labels != normal_label)[0]\n",
    "\n",
    "    selected_normal_indices = np.random.choice(normal_indices, max_normal_samples, replace=False)\n",
    "\n",
    "    # Combine with all other class samples\n",
    "    balanced_indices = np.concatenate([selected_normal_indices, other_indices])\n",
    "\n",
    "    # Extract balanced data\n",
    "    balanced_signals = signals[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    # Print updated label counts\n",
    "    updated_counts = Counter(balanced_labels)\n",
    "    print(\"\\n========== UPDATED ENCODED LABEL COUNTS ==========\")\n",
    "    for encoded_label, count in updated_counts.items():\n",
    "        decoded_label = [k for k, v in label_encoding.items() if v == encoded_label][0]\n",
    "        print(f\"Encoded Label: {encoded_label} ({decoded_label}) - Count: {count}\")\n",
    "    print(\"===================================================\\n\")\n",
    "\n",
    "    return balanced_signals, balanced_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Segmentation\n",
    "segment_length_sec = 10  \n",
    "fs = 250  \n",
    "\n",
    "# Initialize Signal and Label Storage\n",
    "rnn_signals, rnn_labels = [], []\n",
    "\n",
    "# Process Best Leads Data\n",
    "for patient_id, data in best_leads_data.items():\n",
    "    signal = data['signal']\n",
    "    labels = data['labels']\n",
    "    segments, segment_labels = prepare_rnn_data(signal, labels, segment_length_sec, fs)\n",
    "    rnn_signals.extend(segments)\n",
    "    rnn_labels.extend(segment_labels)\n",
    "\n",
    "# Define Label Encoding Dictionary\n",
    "LABEL_ENCODING = {\n",
    "    'normal': 0,\n",
    "    'atrial_fibrillation': 1,\n",
    "    'atrial_flutter': 2,\n",
    "    'ventricular_bigeminy': 3,\n",
    "    'Other': 4  \n",
    "}\n",
    "\n",
    "# Encode Labels After Segmentation\n",
    "rnn_signals = np.array(rnn_signals)\n",
    "rnn_labels_encoded = np.array([LABEL_ENCODING[label] for label in rnn_labels])\n",
    "\n",
    "# Print Encoded Data Information\n",
    "print(f\"Labels Before Encoding (First 10): {rnn_labels[:10]}\")\n",
    "print(f\"Encoded Labels (First 10): {rnn_labels_encoded[:10]}\")\n",
    "print(f\"Segmented Signals Shape: {rnn_signals.shape}\")\n",
    "print(f\"Encoded Labels Shape: {rnn_labels_encoded.shape}\")\n",
    "\n",
    "\n",
    "encoded_label_counts = Counter(rnn_labels_encoded)\n",
    "print(\"\\n========== ENCODED LABEL COUNTS ==========\")\n",
    "for encoded_label, count in encoded_label_counts.items():\n",
    "    decoded_label = [k for k, v in LABEL_ENCODING.items() if v == encoded_label][0]\n",
    "    print(f\"Encoded Label: {encoded_label} ({decoded_label}) - Count: {count}\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# sub sample the normal class, without it just overfits right away.\n",
    "rnn_signals, rnn_labels_encoded = balance_dataset(\n",
    "    signals=rnn_signals, \n",
    "    labels=rnn_labels_encoded, \n",
    "    label_encoding=LABEL_ENCODING, \n",
    "    max_normal_samples=800\n",
    ")\n",
    "\n",
    "\n",
    "# Split Data into Train, Validation, and Test Sets\n",
    "data_splits = split_data(rnn_signals, rnn_labels_encoded)\n",
    "\n",
    "# Create DataLoaders for PyTorch\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_dataloaders(data_splits, batch_size=batch_size)\n",
    "\n",
    "# Print DataLoader Sizes\n",
    "print(f\"Train Loader Size: {len(train_loader)} batches\")\n",
    "print(f\"Validation Loader Size: {len(val_loader)} batches\")\n",
    "print(f\"Test Loader Size: {len(test_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "# input size is just 1 because 1D, output size is the number of classes. RNN might need to be updated later with the num layers,\n",
    "# need to add GRU / LSTM's to it or maybe just an attention mechanism.\n",
    "\n",
    "# below is for the CNNLSTM in the event that sequence length changes\n",
    "sample_data, _ = next(iter(train_loader))\n",
    "sequence_length = sample_data.shape[1]  \n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    'RNN': ECG_RNN(input_size=1, hidden_size=64, num_layers=2, num_classes=5),\n",
    "    # 'RNN2': RNN2(input_size=1, hidden_size=128, num_layers=4, num_classes=5),\n",
    "    #'ResNet': ResNet1D(input_channels=1, num_classes=5), \n",
    "    # 'RNN' : ECG_RNN(input_size=1, hidden_size=128, num_layers=4, num_classes=5), #<--- mod this one if playground -ing \n",
    "    'CNNLSTM': CNNLSTM(input_shape=(1, sequence_length)),\n",
    "    'Seq2Seq': ECGSeq2Seq(input_size=1, hidden_size=1, num_layers=2, num_classes=5), \n",
    "}\n",
    "\n",
    "# Select model to train\n",
    "selected_model_name = ''  # Change this to 'ResNet' or whatever really, \n",
    "#the config might bug out sometiems tho if u dont reset kernels and run the notebook\n",
    "\n",
    "# Initialize model, criterion, and optimizer <-- adam / n-adam worked best, feel free to mess w it\n",
    "model = model_configs[selected_model_name].to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 25\n",
    "\n",
    "# Train the selected model <-- uncomment the below for other models\n",
    "# trained_model = train_model(\n",
    "#     model, train_loader, val_loader, num_epochs=25, criterion=criterion, \n",
    "#     optimizer=optimizer, device=device, patience=5, min_delta=0.001\n",
    "# )\n",
    "\n",
    "# use the below for CNNLSTM as it has its own train.\n",
    "trained_model = train_cnn_lstm(\n",
    "    model, train_loader, val_loader, num_epochs=num_epochs, criterion=criterion, \n",
    "    optimizer=optimizer, device=device, patience=5, min_delta=0.001\n",
    ")\n",
    "\n",
    "# Evaluate the trained model\n",
    "test_acc, test_loss = evaluate_model(trained_model, test_loader, criterion, device)\n",
    "print(f\"{selected_model_name} Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters for segmentation\n",
    "# segment_length_sec = 10  # Segment length in seconds\n",
    "# fs = 250  # Sampling frequency\n",
    "\n",
    "# # Prepare segmented data\n",
    "# rnn_signals = []\n",
    "# rnn_labels = []\n",
    "\n",
    "# for patient_id, data in best_leads_data.items():\n",
    "#     signal = data['signal']  # The best lead's signal\n",
    "#     labels = data['labels']  # Corresponding labels\n",
    "\n",
    "#     # Segment the data\n",
    "#     segments, segment_labels = segment.prepare_rnn_data(signal, labels, segment_length_sec, fs)\n",
    "#     rnn_signals.extend(segments)\n",
    "#     rnn_labels.extend(segment_labels)\n",
    "\n",
    "# # Convert to numpy arrays\n",
    "# rnn_signals = np.array(rnn_signals)\n",
    "# rnn_labels = np.array(rnn_labels)\n",
    "\n",
    "# print(f\"RNN Signals Shape: {rnn_signals.shape}\")\n",
    "# print(f\"RNN Labels Shape: {rnn_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rnn_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # putting it in segment file wasn't working so trying it here \n",
    "# LABEL_ENCODING = {\n",
    "#     'normal': 0,\n",
    "#     'atrial_fibrillation': 1,\n",
    "#     'atrial_flutter': 2,\n",
    "#     'ventricular_bigeminy': 3,\n",
    "#     'Other': 4  \n",
    "# }\n",
    "\n",
    "# print(f\"Labels Before Encoding (First 10): {rnn_labels[:10]}\")\n",
    "\n",
    "# # Encode Labels After Segmentation\n",
    "# rnn_labels_encoded = np.array([LABEL_ENCODING[label] for label in rnn_labels])\n",
    "\n",
    "# print(f\"Encoded Labels (First 10): {rnn_labels_encoded[:10]}\")\n",
    "# print(f\"Segmented Signals Shape: {rnn_signals.shape}\")\n",
    "# print(f\"Encoded Labels Shape: {rnn_labels_encoded.shape}\")\n",
    "\n",
    "# # Split and Create DataLoaders\n",
    "# data_splits = segment.split_data(rnn_signals, rnn_labels_encoded)\n",
    "# train_loader, val_loader, test_loader = dataloaders.create_dataloaders(data_splits, batch_size=32)\n",
    "\n",
    "# # Print DataLoader Sizes to double check, was throwing an issue earlier because I forgot to categorically encode it \n",
    "# print(f\"Train Loader Size: {len(train_loader)} batches\")\n",
    "# print(f\"Validation Loader Size: {len(val_loader)} batches\")\n",
    "# print(f\"Test Loader Size: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into train, validation, and test sets\n",
    "# data_splits = segment.split_data(rnn_signals, rnn_labels)\n",
    "\n",
    "# # Create DataLoaders for PyTorch\n",
    "# train_loader, val_loader, test_loader = dataloaders.create_dataloaders(data_splits, batch_size=32)\n",
    "\n",
    "# print(f\"Train Loader Size: {len(train_loader)} batches\")\n",
    "# print(f\"Validation Loader Size: {len(val_loader)} batches\")\n",
    "# print(f\"Test Loader Size: {len(test_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class ECG_RNN(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, num_layers=2, num_classes=5):\n",
    "#         super(ECG_RNN, self).__init__()\n",
    "#         self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.rnn(x)\n",
    "#         out = out[:, -1, :]  # Take the last hidden state\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device):\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss, correct, total = 0.0, 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 val_loss += criterion(outputs, labels).item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "#               f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "#               f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "#               f\"Val Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def evaluate_model(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# input_size = 1  # ECG data is 1D\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# num_classes = len(set(rnn_labels))\n",
    "# num_epochs = 20\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Create the model\n",
    "# model = ECG_RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Train the model\n",
    "# train_loader, val_loader, test_loader = dataloaders.create_dataloaders(data_splits, batch_size)\n",
    "# model = train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluate_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
